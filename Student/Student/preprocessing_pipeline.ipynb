{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T09:24:55.137373Z",
     "start_time": "2025-12-14T09:24:54.156880Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import warnings\n",
    "import os\n",
    "import joblib\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style for visualizations\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "class StudentDepressionPreprocessor:\n",
    "    def __init__(self, input_file_path, output_dir='data/processed'):\n",
    "        self.input_file = input_file_path\n",
    "        self.output_dir = output_dir\n",
    "        self.raw_data = None\n",
    "        self.cleaned_data = None\n",
    "        self.preprocessed_data = None\n",
    "        self.setup_directories()\n",
    "\n",
    "    def setup_directories(self):\n",
    "        \"\"\"Create necessary directories\"\"\"\n",
    "        dirs = [self.output_dir, 'visualizations', 'models']\n",
    "        for dir_path in dirs:\n",
    "            os.makedirs(dir_path, exist_ok=True)\n",
    "        print(\"‚úì Directory structure created\")\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load dataset from CSV or Excel\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"LOADING DATA\")\n",
    "        print(\"=\"*60)\n",
    "        try:\n",
    "            if self.input_file.endswith('.xlsx'):\n",
    "                self.raw_data = pd.read_excel(self.input_file)\n",
    "            else:\n",
    "                self.raw_data = pd.read_csv(self.input_file)\n",
    "\n",
    "            print(f\"‚úì Data loaded successfully\")\n",
    "            print(f\"  Shape: {self.raw_data.shape}\")\n",
    "            print(f\"  Columns: {list(self.raw_data.columns)}\")\n",
    "            return self.raw_data\n",
    "        except Exception as e:\n",
    "            print(f\"‚úó Error loading data: {e}\")\n",
    "            return None\n",
    "\n",
    "    def clean_data(self):\n",
    "        \"\"\"Clean and prepare the dataset\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"CLEANING DATA\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        self.cleaned_data = self.raw_data.copy()\n",
    "        initial_shape = self.cleaned_data.shape\n",
    "\n",
    "        # 1. Handle Target Variable (Depression)\n",
    "        if 'Depression' in self.cleaned_data.columns:\n",
    "            print(\"\\n1. Processing Target Variable (Depression):\")\n",
    "            unique_vals = sorted(self.cleaned_data['Depression'].unique())\n",
    "            print(f\"   Original unique values: {unique_vals}\")\n",
    "            print(f\"   Value counts:\\n{self.cleaned_data['Depression'].value_counts().sort_index()}\")\n",
    "\n",
    "            # Map common encodings to binary 0/1\n",
    "            # -1 or 2 sometimes used for \"Healthy\", 0 or 1 for \"Depressed\"\n",
    "            if set(unique_vals) == {-1, 1}:\n",
    "                print(\"   ‚Üí Mapping: -1‚Üí0 (Healthy), 1‚Üí1 (Depressed)\")\n",
    "                self.cleaned_data['Depression'] = self.cleaned_data['Depression'].map({-1: 0, 1: 1})\n",
    "            elif set(unique_vals) == {1, 2}:\n",
    "                print(\"   ‚Üí Mapping: 1‚Üí0 (Healthy), 2‚Üí1 (Depressed)\")\n",
    "                self.cleaned_data['Depression'] = self.cleaned_data['Depression'].map({1: 0, 2: 1})\n",
    "            elif set(unique_vals) == {0, 1}:\n",
    "                print(\"   ‚Üí Already binary (0, 1). No mapping needed.\")\n",
    "            else:\n",
    "                print(f\"   ‚ö†Ô∏è  WARNING: Unexpected values {unique_vals}\")\n",
    "                print(\"   ‚Üí Attempting to convert to binary (0=healthy, 1=depressed)\")\n",
    "                # Assume smallest value is healthy (0), largest is depressed (1)\n",
    "                min_val = min(unique_vals)\n",
    "                self.cleaned_data['Depression'] = (self.cleaned_data['Depression'] != min_val).astype(int)\n",
    "\n",
    "            final_vals = sorted(self.cleaned_data['Depression'].unique())\n",
    "            final_counts = self.cleaned_data['Depression'].value_counts().sort_index()\n",
    "            print(f\"   Final unique values: {final_vals}\")\n",
    "            print(f\"   Final value counts:\\n{final_counts}\")\n",
    "\n",
    "            if len(final_vals) < 2:\n",
    "                print(\"   ‚úó ERROR: Only one class in target! Check your raw data file.\")\n",
    "                return None\n",
    "\n",
    "        # 2. Remove duplicates\n",
    "        print(\"\\n2. Removing Duplicates:\")\n",
    "        duplicates = self.cleaned_data.duplicated().sum()\n",
    "        self.cleaned_data = self.cleaned_data.drop_duplicates()\n",
    "        print(f\"   Removed {duplicates} duplicate rows\")\n",
    "\n",
    "        # 3. Handle missing values\n",
    "        print(\"\\n3. Handling Missing Values:\")\n",
    "        missing_before = self.cleaned_data.isnull().sum().sum()\n",
    "\n",
    "        numeric_cols = self.cleaned_data.select_dtypes(include=[np.number]).columns\n",
    "        categorical_cols = self.cleaned_data.select_dtypes(include=['object']).columns\n",
    "\n",
    "        # Fill numeric with median\n",
    "        for col in numeric_cols:\n",
    "            if col != 'Depression':  # Don't fill target variable\n",
    "                missing = self.cleaned_data[col].isnull().sum()\n",
    "                if missing > 0:\n",
    "                    self.cleaned_data[col] = self.cleaned_data[col].fillna(\n",
    "                        self.cleaned_data[col].median()\n",
    "                    )\n",
    "                    print(f\"   {col}: Filled {missing} missing values with median\")\n",
    "\n",
    "        # Fill categorical with mode\n",
    "        for col in categorical_cols:\n",
    "            missing = self.cleaned_data[col].isnull().sum()\n",
    "            if missing > 0:\n",
    "                mode_value = self.cleaned_data[col].mode()\n",
    "                if len(mode_value) > 0:\n",
    "                    self.cleaned_data[col] = self.cleaned_data[col].fillna(mode_value[0])\n",
    "                    print(f\"   {col}: Filled {missing} missing values with mode\")\n",
    "\n",
    "        missing_after = self.cleaned_data.isnull().sum().sum()\n",
    "        print(f\"   Total missing values: {missing_before} ‚Üí {missing_after}\")\n",
    "\n",
    "        # 4. Standardize text in categorical columns\n",
    "        print(\"\\n4. Standardizing Text:\")\n",
    "        for col in categorical_cols:\n",
    "            self.cleaned_data[col] = (\n",
    "                self.cleaned_data[col]\n",
    "                .astype(str)\n",
    "                .str.strip()\n",
    "                .str.title()\n",
    "            )\n",
    "        print(f\"   Standardized {len(categorical_cols)} categorical columns\")\n",
    "\n",
    "        # 5. Remove unnecessary columns\n",
    "        print(\"\\n5. Removing Unnecessary Columns:\")\n",
    "        cols_to_drop = ['City', 'id', 'ID', 'Unnamed: 0']\n",
    "        dropped = []\n",
    "        for col in cols_to_drop:\n",
    "            if col in self.cleaned_data.columns:\n",
    "                self.cleaned_data = self.cleaned_data.drop(columns=[col])\n",
    "                dropped.append(col)\n",
    "        if dropped:\n",
    "            print(f\"   Dropped: {dropped}\")\n",
    "        else:\n",
    "            print(\"   No unnecessary columns found\")\n",
    "\n",
    "        # 6. Save cleaned data\n",
    "        output_path = f'{self.output_dir}/cleaned_data.csv'\n",
    "        self.cleaned_data.to_csv(output_path, index=False)\n",
    "        print(f\"\\n‚úì Cleaned data saved to: {output_path}\")\n",
    "        print(f\"  Final shape: {initial_shape} ‚Üí {self.cleaned_data.shape}\")\n",
    "\n",
    "        return self.cleaned_data\n",
    "\n",
    "    def preprocess_for_ml(self):\n",
    "        \"\"\"Preprocess data for machine learning\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PREPROCESSING FOR MACHINE LEARNING\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        if self.cleaned_data is None:\n",
    "            print(\"‚úó No cleaned data available. Run clean_data() first.\")\n",
    "            return None\n",
    "\n",
    "        self.preprocessed_data = self.cleaned_data.copy()\n",
    "        target_col = 'Depression'\n",
    "\n",
    "        # Separate features and target\n",
    "        if target_col not in self.preprocessed_data.columns:\n",
    "            print(f\"‚úó Target column '{target_col}' not found!\")\n",
    "            return None\n",
    "\n",
    "        # Identify column types\n",
    "        categorical_cols = self.preprocessed_data.select_dtypes(\n",
    "            include=['object']\n",
    "        ).columns.tolist()\n",
    "\n",
    "        numeric_cols = self.preprocessed_data.select_dtypes(\n",
    "            include=[np.number]\n",
    "        ).columns.tolist()\n",
    "\n",
    "        # Remove target from numeric columns list\n",
    "        if target_col in numeric_cols:\n",
    "            numeric_cols.remove(target_col)\n",
    "\n",
    "        print(f\"\\n1. Column Types:\")\n",
    "        print(f\"   Numeric features: {len(numeric_cols)}\")\n",
    "        print(f\"   Categorical features: {len(categorical_cols)}\")\n",
    "        print(f\"   Target: {target_col}\")\n",
    "\n",
    "        # 2. One-Hot Encoding for Categorical Variables\n",
    "        if categorical_cols:\n",
    "            print(f\"\\n2. One-Hot Encoding:\")\n",
    "            print(f\"   Encoding {len(categorical_cols)} columns...\")\n",
    "\n",
    "            X_categorical = self.preprocessed_data[categorical_cols]\n",
    "            X_encoded = pd.get_dummies(X_categorical, drop_first=True, dtype=int)\n",
    "\n",
    "            # Create encoding map\n",
    "            encoding_map = {}\n",
    "            for col in categorical_cols:\n",
    "                new_cols = [c for c in X_encoded.columns if c.startswith(col + '_')]\n",
    "                encoding_map[col] = new_cols\n",
    "                print(f\"   {col} ‚Üí {len(new_cols)} columns\")\n",
    "\n",
    "            # Save encoding map\n",
    "            encoding_path = f'{self.output_dir}/encoding_map.json'\n",
    "            with open(encoding_path, 'w') as f:\n",
    "                json.dump(encoding_map, f, indent=2)\n",
    "            print(f\"   ‚úì Encoding map saved\")\n",
    "\n",
    "            # Add encoded columns and remove original\n",
    "            self.preprocessed_data = pd.concat(\n",
    "                [self.preprocessed_data.drop(columns=categorical_cols), X_encoded],\n",
    "                axis=1\n",
    "            )\n",
    "\n",
    "        # 3. Scale Numeric Features\n",
    "        if numeric_cols:\n",
    "            print(f\"\\n3. Scaling Numeric Features:\")\n",
    "            print(f\"   Scaling {len(numeric_cols)} columns...\")\n",
    "\n",
    "            scaler = StandardScaler()\n",
    "            self.preprocessed_data[numeric_cols] = scaler.fit_transform(\n",
    "                self.preprocessed_data[numeric_cols]\n",
    "            )\n",
    "\n",
    "            # Save scaler\n",
    "            scaler_path = f'{self.output_dir}/scaler.pkl'\n",
    "            joblib.dump(scaler, scaler_path)\n",
    "            print(f\"   ‚úì Scaler saved\")\n",
    "\n",
    "        # 4. Save feature names\n",
    "        feature_cols = [c for c in self.preprocessed_data.columns if c != target_col]\n",
    "        feature_path = f'{self.output_dir}/feature_names.json'\n",
    "        with open(feature_path, 'w') as f:\n",
    "            json.dump(feature_cols, f, indent=2)\n",
    "        print(f\"\\n4. Feature Names:\")\n",
    "        print(f\"   Total features: {len(feature_cols)}\")\n",
    "        print(f\"   ‚úì Feature names saved\")\n",
    "\n",
    "        # 5. Save preprocessed data\n",
    "        output_path = f'{self.output_dir}/preprocessed_data.csv'\n",
    "        self.preprocessed_data.to_csv(output_path, index=False)\n",
    "        print(f\"\\n‚úì Preprocessed data saved to: {output_path}\")\n",
    "        print(f\"  Final shape: {self.preprocessed_data.shape}\")\n",
    "\n",
    "        return self.preprocessed_data\n",
    "\n",
    "    def generate_summary_report(self):\n",
    "        \"\"\"Generate a summary report of the preprocessing\"\"\"\n",
    "        print(\"\\n\" + \"=\"*60)\n",
    "        print(\"PREPROCESSING SUMMARY\")\n",
    "        print(\"=\"*60)\n",
    "\n",
    "        if self.preprocessed_data is None:\n",
    "            print(\"No preprocessed data available\")\n",
    "            return\n",
    "\n",
    "        summary = {\n",
    "            \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "            \"raw_data_shape\": self.raw_data.shape if self.raw_data is not None else None,\n",
    "            \"cleaned_data_shape\": self.cleaned_data.shape if self.cleaned_data is not None else None,\n",
    "            \"preprocessed_data_shape\": self.preprocessed_data.shape,\n",
    "            \"target_distribution\": self.preprocessed_data['Depression'].value_counts().to_dict() if 'Depression' in self.preprocessed_data.columns else None,\n",
    "        }\n",
    "\n",
    "        summary_path = f'{self.output_dir}/preprocessing_summary.json'\n",
    "        with open(summary_path, 'w') as f:\n",
    "            json.dump(summary, f, indent=2)\n",
    "\n",
    "        print(\"Final Dataset Information:\")\n",
    "        print(f\"  Samples: {summary['preprocessed_data_shape'][0]}\")\n",
    "        print(f\"  Features: {summary['preprocessed_data_shape'][1] - 1}\")  # -1 for target\n",
    "        if summary['target_distribution']:\n",
    "            print(f\"  Target Distribution:\")\n",
    "            for k, v in summary['target_distribution'].items():\n",
    "                label = \"Healthy\" if k == 0 else \"Depressed\"\n",
    "                print(f\"    {label} ({k}): {v} ({v/sum(summary['target_distribution'].values())*100:.1f}%)\")\n",
    "\n",
    "        print(f\"\\n‚úì Summary saved to: {summary_path}\")\n",
    "\n",
    "    def run_pipeline(self):\n",
    "        \"\"\"Run the complete preprocessing pipeline\"\"\"\n",
    "        print(\"\\n\" + \"üîÑ STARTING PREPROCESSING PIPELINE üîÑ\".center(60))\n",
    "\n",
    "        # Load data\n",
    "        if self.load_data() is None:\n",
    "            print(\"\\n‚úó Pipeline failed: Could not load data\")\n",
    "            return False\n",
    "\n",
    "        # Clean data\n",
    "        if self.clean_data() is None:\n",
    "            print(\"\\n‚úó Pipeline failed: Could not clean data\")\n",
    "            return False\n",
    "\n",
    "        # Preprocess for ML\n",
    "        if self.preprocess_for_ml() is None:\n",
    "            print(\"\\n‚úó Pipeline failed: Could not preprocess data\")\n",
    "            return False\n",
    "\n",
    "        # Generate summary\n",
    "        self.generate_summary_report()\n",
    "\n",
    "        print(\"\\n\" + \"‚úÖ PREPROCESSING COMPLETED SUCCESSFULLY! ‚úÖ\".center(60))\n",
    "        print(\"\\nOutput files created:\")\n",
    "        print(f\"  üìÅ {self.output_dir}/cleaned_data.csv\")\n",
    "        print(f\"  üìÅ {self.output_dir}/preprocessed_data.csv\")\n",
    "        print(f\"  üìÅ {self.output_dir}/scaler.pkl\")\n",
    "        print(f\"  üìÅ {self.output_dir}/encoding_map.json\")\n",
    "        print(f\"  üìÅ {self.output_dir}/feature_names.json\")\n",
    "        print(f\"  üìÅ {self.output_dir}/preprocessing_summary.json\")\n",
    "\n",
    "        return True\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # IMPORTANT: Update this path to match your file location\n",
    "    INPUT_FILE = \"data/raw/student_depression_dataset (1).csv\"\n",
    "    OUTPUT_DIR = \"data/processed\"\n",
    "\n",
    "    # Create preprocessor and run\n",
    "    preprocessor = StudentDepressionPreprocessor(INPUT_FILE, OUTPUT_DIR)\n",
    "    success = preprocessor.run_pipeline()\n",
    "\n",
    "    if success:\n",
    "        print(\"\\nüéâ Ready for model training!\")\n",
    "    else:\n",
    "        print(\"\\n‚ö†Ô∏è  Please check the errors above and fix your data.\")"
   ],
   "id": "1ae60dbcd3578e01",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Directory structure created\n",
      "\n",
      "            üîÑ STARTING PREPROCESSING PIPELINE üîÑ             \n",
      "\n",
      "============================================================\n",
      "LOADING DATA\n",
      "============================================================\n",
      "‚úì Data loaded successfully\n",
      "  Shape: (27901, 18)\n",
      "  Columns: ['id', 'Gender', 'Age', 'City', 'Profession', 'Academic Pressure', 'Work Pressure', 'CGPA', 'Study Satisfaction', 'Job Satisfaction', 'Sleep Duration', 'Dietary Habits', 'Degree', 'Have you ever had suicidal thoughts ?', 'Work/Study Hours', 'Financial Stress', 'Family History of Mental Illness', 'Depression']\n",
      "\n",
      "============================================================\n",
      "CLEANING DATA\n",
      "============================================================\n",
      "\n",
      "1. Processing Target Variable (Depression):\n",
      "   Original unique values: [np.int64(0), np.int64(1)]\n",
      "   Value counts:\n",
      "Depression\n",
      "0    11565\n",
      "1    16336\n",
      "Name: count, dtype: int64\n",
      "   ‚Üí Already binary (0, 1). No mapping needed.\n",
      "   Final unique values: [np.int64(0), np.int64(1)]\n",
      "   Final value counts:\n",
      "Depression\n",
      "0    11565\n",
      "1    16336\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2. Removing Duplicates:\n",
      "   Removed 0 duplicate rows\n",
      "\n",
      "3. Handling Missing Values:\n",
      "   Total missing values: 0 ‚Üí 0\n",
      "\n",
      "4. Standardizing Text:\n",
      "   Standardized 9 categorical columns\n",
      "\n",
      "5. Removing Unnecessary Columns:\n",
      "   Dropped: ['City', 'id']\n",
      "\n",
      "‚úì Cleaned data saved to: data/processed/cleaned_data.csv\n",
      "  Final shape: (27901, 18) ‚Üí (27901, 16)\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING FOR MACHINE LEARNING\n",
      "============================================================\n",
      "\n",
      "1. Column Types:\n",
      "   Numeric features: 7\n",
      "   Categorical features: 8\n",
      "   Target: Depression\n",
      "\n",
      "2. One-Hot Encoding:\n",
      "   Encoding 8 columns...\n",
      "   Gender ‚Üí 1 columns\n",
      "   Profession ‚Üí 13 columns\n",
      "   Sleep Duration ‚Üí 4 columns\n",
      "   Dietary Habits ‚Üí 3 columns\n",
      "   Degree ‚Üí 27 columns\n",
      "   Have you ever had suicidal thoughts ? ‚Üí 1 columns\n",
      "   Financial Stress ‚Üí 5 columns\n",
      "   Family History of Mental Illness ‚Üí 1 columns\n",
      "   ‚úì Encoding map saved\n",
      "\n",
      "3. Scaling Numeric Features:\n",
      "   Scaling 7 columns...\n",
      "   ‚úì Scaler saved\n",
      "\n",
      "4. Feature Names:\n",
      "   Total features: 62\n",
      "   ‚úì Feature names saved\n",
      "\n",
      "‚úì Preprocessed data saved to: data/processed/preprocessed_data.csv\n",
      "  Final shape: (27901, 63)\n",
      "\n",
      "============================================================\n",
      "PREPROCESSING SUMMARY\n",
      "============================================================\n",
      "Final Dataset Information:\n",
      "  Samples: 27901\n",
      "  Features: 62\n",
      "  Target Distribution:\n",
      "    Depressed (1): 16336 (58.5%)\n",
      "    Healthy (0): 11565 (41.5%)\n",
      "\n",
      "‚úì Summary saved to: data/processed/preprocessing_summary.json\n",
      "\n",
      "         ‚úÖ PREPROCESSING COMPLETED SUCCESSFULLY! ‚úÖ          \n",
      "\n",
      "Output files created:\n",
      "  üìÅ data/processed/cleaned_data.csv\n",
      "  üìÅ data/processed/preprocessed_data.csv\n",
      "  üìÅ data/processed/scaler.pkl\n",
      "  üìÅ data/processed/encoding_map.json\n",
      "  üìÅ data/processed/feature_names.json\n",
      "  üìÅ data/processed/preprocessing_summary.json\n",
      "\n",
      "üéâ Ready for model training!\n"
     ]
    }
   ],
   "execution_count": 1
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
