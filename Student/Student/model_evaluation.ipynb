{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-12-14T10:15:37.531757Z",
     "start_time": "2025-12-14T10:14:51.968076Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.model_selection import (\n",
    "    train_test_split, RandomizedSearchCV, StratifiedKFold\n",
    ")\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, f1_score, confusion_matrix,\n",
    "    roc_curve, roc_auc_score, recall_score, precision_score,\n",
    "    matthews_corrcoef\n",
    ")\n",
    "import joblib\n",
    "import warnings\n",
    "import os\n",
    "import json\n",
    "from datetime import datetime\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 8)\n",
    "\n",
    "class ModelEvaluator:\n",
    "    def __init__(self, data_path, target_column='Depression', output_dir='evaluation_results'):\n",
    "        self.data_path = data_path\n",
    "        self.target_column = target_column\n",
    "        self.output_dir = output_dir\n",
    "        self.data = None\n",
    "        self.X_train = None\n",
    "        self.X_test = None\n",
    "        self.y_train = None\n",
    "        self.y_test = None\n",
    "        self.tuned_models = {}\n",
    "        self.best_params = {}\n",
    "        self.evaluation_results = {}\n",
    "        self.best_model = None\n",
    "        self.best_model_name = None\n",
    "\n",
    "        # Create output directory structure\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        os.makedirs(f'{self.output_dir}/visualizations', exist_ok=True)\n",
    "        os.makedirs(f'{self.output_dir}/models', exist_ok=True)\n",
    "        os.makedirs(f'{self.output_dir}/reports', exist_ok=True)\n",
    "\n",
    "    def load_data(self):\n",
    "        \"\"\"Load and prepare data with safer Label Fixing\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"LOADING DATA\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        try:\n",
    "            if not os.path.exists(self.data_path):\n",
    "                print(f\"ERROR: File not found at {self.data_path}\")\n",
    "                return False\n",
    "\n",
    "            self.data = pd.read_csv(self.data_path)\n",
    "            print(f\"Data loaded: {self.data.shape}\")\n",
    "\n",
    "            # Check if target column exists\n",
    "            if self.target_column not in self.data.columns:\n",
    "                print(f\"ERROR: Target column '{self.target_column}' not found!\")\n",
    "                return False\n",
    "\n",
    "            # ---------------------------------------------------------\n",
    "            # TARGET FIXING (Safer Logic)\n",
    "            # ---------------------------------------------------------\n",
    "            print(\"Checking target labels...\")\n",
    "            unique_vals = sorted(self.data[self.target_column].unique())\n",
    "            print(f\"  Raw unique values: {unique_vals}\")\n",
    "\n",
    "            # Only map -1 to 0. Leave 0 and 1 alone.\n",
    "            if -1 in unique_vals:\n",
    "                print(\"  Found -1. Mapping to 0 (Healthy).\")\n",
    "                self.data[self.target_column] = self.data[self.target_column].replace({-1: 0})\n",
    "\n",
    "            # Verify we have at least 2 classes\n",
    "            final_counts = self.data[self.target_column].value_counts()\n",
    "            print(f\"  Final Distribution:\\n{final_counts}\")\n",
    "\n",
    "            if len(final_counts) < 2:\n",
    "                print(\"CRITICAL ERROR: Dataset has only 1 class! Models cannot train.\")\n",
    "                print(\"   Please check your raw data or preprocessing logic.\")\n",
    "                return False\n",
    "            # ---------------------------------------------------------\n",
    "\n",
    "            # Prepare features and target\n",
    "            X = self.data.drop(columns=[self.target_column])\n",
    "            y = self.data[self.target_column]\n",
    "\n",
    "            # Split data\n",
    "            self.X_train, self.X_test, self.y_train, self.y_test = train_test_split(\n",
    "                X, y, test_size=0.2, random_state=42, stratify=y\n",
    "            )\n",
    "\n",
    "            print(f\"Train shape: {self.X_train.shape}\")\n",
    "            print(f\"Test shape: {self.X_test.shape}\")\n",
    "            print(f\"Train class distribution: {self.y_train.value_counts().to_dict()}\")\n",
    "            print(f\"Test class distribution: {self.y_test.value_counts().to_dict()}\")\n",
    "            return True\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"CRITICAL ERROR loading data: {e}\")\n",
    "            return False\n",
    "\n",
    "    def get_param_grids(self):\n",
    "        \"\"\"Define hyperparameter grids\"\"\"\n",
    "        return {\n",
    "            'Logistic Regression': {\n",
    "                'C': [0.01, 0.1, 1, 10],\n",
    "                'solver': ['liblinear'],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'Random Forest': {\n",
    "                'n_estimators': [50, 100, 200],\n",
    "                'max_depth': [10, 20, None],\n",
    "                'min_samples_leaf': [1, 2, 4],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'Gradient Boosting': {\n",
    "                'n_estimators': [50, 100],\n",
    "                'learning_rate': [0.05, 0.1, 0.2],\n",
    "                'max_depth': [3, 5]\n",
    "            },\n",
    "            'K-Nearest Neighbors': {\n",
    "                'n_neighbors': [3, 5, 7, 9],\n",
    "                'weights': ['uniform', 'distance']\n",
    "            },\n",
    "            'Decision Tree': {\n",
    "                'max_depth': [5, 10, None],\n",
    "                'min_samples_leaf': [2, 5],\n",
    "                'class_weight': ['balanced', None]\n",
    "            },\n",
    "            'Naive Bayes': {\n",
    "                'var_smoothing': [1e-9, 1e-8]\n",
    "            }\n",
    "        }\n",
    "\n",
    "    def hyperparameter_tuning(self, n_iter=10):\n",
    "        \"\"\"Perform hyperparameter tuning with error catching\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(f\"HYPERPARAMETER TUNING\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        base_models = {\n",
    "            'Logistic Regression': LogisticRegression(random_state=42, max_iter=1000),\n",
    "            'Random Forest': RandomForestClassifier(random_state=42),\n",
    "            'Gradient Boosting': GradientBoostingClassifier(random_state=42),\n",
    "            'K-Nearest Neighbors': KNeighborsClassifier(),\n",
    "            'Decision Tree': DecisionTreeClassifier(random_state=42),\n",
    "            'Naive Bayes': GaussianNB()\n",
    "        }\n",
    "\n",
    "        param_grids = self.get_param_grids()\n",
    "        cv_splitter = StratifiedKFold(n_splits=3, shuffle=True, random_state=42)\n",
    "\n",
    "        for model_name, base_model in base_models.items():\n",
    "            print(f\"\\nTraining {model_name}...\")\n",
    "\n",
    "            try:\n",
    "                search = RandomizedSearchCV(\n",
    "                    base_model,\n",
    "                    param_distributions=param_grids[model_name],\n",
    "                    n_iter=n_iter,\n",
    "                    cv=cv_splitter,\n",
    "                    scoring='f1',\n",
    "                    n_jobs=-1,\n",
    "                    random_state=42,\n",
    "                    error_score='raise'\n",
    "                )\n",
    "\n",
    "                search.fit(self.X_train, self.y_train)\n",
    "\n",
    "                self.tuned_models[model_name] = search.best_estimator_\n",
    "                self.best_params[model_name] = search.best_params_\n",
    "                print(f\"  Success! Best F1 Score: {search.best_score_:.4f}\")\n",
    "                print(f\"  Best Parameters: {search.best_params_}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"  FAILED: {str(e)}\")\n",
    "                continue\n",
    "\n",
    "        if not self.tuned_models:\n",
    "            print(\"\\nCRITICAL: No models were trained successfully.\")\n",
    "            return False\n",
    "\n",
    "        print(f\"\\nSuccessfully trained {len(self.tuned_models)} models\")\n",
    "        return True\n",
    "\n",
    "    def evaluate_models(self):\n",
    "        \"\"\"Evaluate all successfully tuned models with MEDICAL METRICS\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"EVALUATING MODELS (Medical Metrics)\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        for model_name, model in self.tuned_models.items():\n",
    "            try:\n",
    "                y_pred = model.predict(self.X_test)\n",
    "                y_pred_train = model.predict(self.X_train)\n",
    "\n",
    "                # Probabilities for ROC\n",
    "                try:\n",
    "                    y_prob = model.predict_proba(self.X_test)[:, 1]\n",
    "                    roc_auc = roc_auc_score(self.y_test, y_prob)\n",
    "                except:\n",
    "                    y_prob = None\n",
    "                    roc_auc = 0.5\n",
    "\n",
    "                # MEDICAL METRICS (Critical for Healthcare)\n",
    "                f1 = f1_score(self.y_test, y_pred)\n",
    "\n",
    "                # Class-specific metrics (pos_label=1 for Depressed)\n",
    "                recall_depressed = recall_score(self.y_test, y_pred, pos_label=1)\n",
    "                precision_depressed = precision_score(self.y_test, y_pred, pos_label=1, zero_division=0)\n",
    "\n",
    "                # Class-specific metrics (pos_label=0 for Healthy)\n",
    "                recall_healthy = recall_score(self.y_test, y_pred, pos_label=0)\n",
    "                precision_healthy = precision_score(self.y_test, y_pred, pos_label=0, zero_division=0)\n",
    "\n",
    "                # Matthews Correlation Coefficient (Best for imbalanced data)\n",
    "                mcc = matthews_corrcoef(self.y_test, y_pred)\n",
    "\n",
    "                # Confusion Matrix values\n",
    "                cm = confusion_matrix(self.y_test, y_pred)\n",
    "                tn, fp, fn, tp = cm.ravel()\n",
    "\n",
    "                # Specificity (True Negative Rate)\n",
    "                specificity = tn / (tn + fp) if (tn + fp) > 0 else 0\n",
    "\n",
    "                self.evaluation_results[model_name] = {\n",
    "                    'model': model,\n",
    "                    'test_accuracy': accuracy_score(self.y_test, y_pred),\n",
    "                    'train_accuracy': accuracy_score(self.y_train, y_pred_train),\n",
    "                    'f1_score': f1,\n",
    "                    'mcc': mcc,\n",
    "                    'recall_depressed': recall_depressed,\n",
    "                    'precision_depressed': precision_depressed,\n",
    "                    'recall_healthy': recall_healthy,\n",
    "                    'precision_healthy': precision_healthy,\n",
    "                    'specificity': specificity,\n",
    "                    'roc_auc': roc_auc,\n",
    "                    'overfit_gap': accuracy_score(self.y_train, y_pred_train) - accuracy_score(self.y_test, y_pred),\n",
    "                    'confusion_matrix': cm,\n",
    "                    'tp': tp, 'tn': tn, 'fp': fp, 'fn': fn,\n",
    "                    'y_pred': y_pred,\n",
    "                    'y_prob': y_prob\n",
    "                }\n",
    "\n",
    "                print(f\"\\n{model_name}:\")\n",
    "                print(f\"  Accuracy: {accuracy_score(self.y_test, y_pred):.3f}\")\n",
    "                print(f\"  F1 Score: {f1:.3f}\")\n",
    "                print(f\"  MCC: {mcc:.3f}\")\n",
    "                print(f\"  Recall (Depressed): {recall_depressed:.3f} [CRITICAL]\")\n",
    "                print(f\"  Precision (Depressed): {precision_depressed:.3f}\")\n",
    "                print(f\"  Specificity: {specificity:.3f}\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error evaluating {model_name}: {e}\")\n",
    "\n",
    "    def compare_models(self):\n",
    "        \"\"\"Compare models with comprehensive medical metrics\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"FINAL RESULTS - MODEL COMPARISON\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        if not self.evaluation_results:\n",
    "            print(\"No evaluation results found. Pipeline failed.\")\n",
    "            return False\n",
    "\n",
    "        comparison_data = []\n",
    "        for name, res in self.evaluation_results.items():\n",
    "            comparison_data.append({\n",
    "                'Model': name,\n",
    "                'Accuracy': res['test_accuracy'],\n",
    "                'F1': res['f1_score'],\n",
    "                'MCC': res['mcc'],\n",
    "                'Recall_Depressed': res['recall_depressed'],\n",
    "                'Precision_Depressed': res['precision_depressed'],\n",
    "                'Specificity': res['specificity'],\n",
    "                'ROC_AUC': res['roc_auc'],\n",
    "                'Overfit_Gap': res['overfit_gap']\n",
    "            })\n",
    "\n",
    "        comparison_df = pd.DataFrame(comparison_data)\n",
    "\n",
    "        if comparison_df.empty:\n",
    "            print(\"Comparison table is empty.\")\n",
    "            return False\n",
    "\n",
    "        # Sort by F1 Score (primary), then by Recall_Depressed (secondary)\n",
    "        comparison_df = comparison_df.sort_values(['F1', 'Recall_Depressed'], ascending=False)\n",
    "\n",
    "        print(\"\\nCOMPLETE MODEL COMPARISON:\")\n",
    "        print(comparison_df.to_string(index=False))\n",
    "\n",
    "        # Save to CSV\n",
    "        comparison_df.to_csv(f'{self.output_dir}/model_comparison.csv', index=False)\n",
    "\n",
    "        # Set best model\n",
    "        self.best_model_name = comparison_df.iloc[0]['Model']\n",
    "        self.best_model = self.evaluation_results[self.best_model_name]['model']\n",
    "\n",
    "        print(f\"\\nWINNER: {self.best_model_name}\")\n",
    "        print(f\"   F1 Score: {comparison_df.iloc[0]['F1']:.3f}\")\n",
    "        print(f\"   Recall (Depressed): {comparison_df.iloc[0]['Recall_Depressed']:.3f}\")\n",
    "        print(f\"   MCC: {comparison_df.iloc[0]['MCC']:.3f}\")\n",
    "\n",
    "        # Medical interpretation\n",
    "        best_recall = comparison_df.iloc[0]['Recall_Depressed']\n",
    "        if best_recall < 0.7:\n",
    "            print(f\"\\nWARNING: Best model recall is {best_recall:.1%}\")\n",
    "            print(\"   This model may miss many depressed students.\")\n",
    "            print(\"   Consider: (1) Collecting more data, (2) Feature engineering, (3) Threshold tuning\")\n",
    "        else:\n",
    "            print(f\"\\nGood recall ({best_recall:.1%}) - Model catches most depressed cases\")\n",
    "\n",
    "        return True\n",
    "\n",
    "    def plot_visualizations(self):\n",
    "        \"\"\"Generate comprehensive visualizations\"\"\"\n",
    "        if not self.evaluation_results:\n",
    "            return\n",
    "\n",
    "        # 1. Confusion Matrices with Medical Context\n",
    "        n_models = len(self.evaluation_results)\n",
    "        rows = (n_models + 2) // 3\n",
    "        fig, axes = plt.subplots(rows, 3, figsize=(15, 5*rows))\n",
    "        axes = axes.flatten() if n_models > 1 else [axes]\n",
    "\n",
    "        for idx, (name, res) in enumerate(self.evaluation_results.items()):\n",
    "            cm = res['confusion_matrix']\n",
    "            sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', ax=axes[idx],\n",
    "                        xticklabels=['Healthy', 'Depressed'],\n",
    "                        yticklabels=['Healthy', 'Depressed'])\n",
    "\n",
    "            # Add medical context to title\n",
    "            recall_dep = res['recall_depressed']\n",
    "            axes[idx].set_title(\n",
    "                f\"{name}\\n\"\n",
    "                f\"Recall (Depressed): {recall_dep:.2f} | \"\n",
    "                f\"MCC: {res['mcc']:.2f}\\n\"\n",
    "                f\"FN={res['fn']} (Missed Cases)\"\n",
    "            )\n",
    "            axes[idx].set_ylabel('Actual')\n",
    "            axes[idx].set_xlabel('Predicted')\n",
    "\n",
    "        for idx in range(n_models, len(axes)):\n",
    "            axes[idx].axis('off')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/visualizations/confusion_matrices.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved confusion matrices\")\n",
    "\n",
    "        # 2. ROC Curves\n",
    "        plt.figure(figsize=(10, 8))\n",
    "        for name, res in self.evaluation_results.items():\n",
    "            if res['y_prob'] is not None:\n",
    "                fpr, tpr, _ = roc_curve(self.y_test, res['y_prob'])\n",
    "                plt.plot(fpr, tpr, label=f'{name} (AUC={res[\"roc_auc\"]:.2f})', linewidth=2)\n",
    "\n",
    "        plt.plot([0, 1], [0, 1], 'k--', label='Random Classifier')\n",
    "        plt.xlabel('False Positive Rate')\n",
    "        plt.ylabel('True Positive Rate (Recall)')\n",
    "        plt.title('ROC Curves - Depression Detection')\n",
    "        plt.legend(loc='lower right')\n",
    "        plt.grid(alpha=0.3)\n",
    "        plt.savefig(f'{self.output_dir}/visualizations/roc_curves.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved ROC curves\")\n",
    "\n",
    "        # 3. Medical Metrics Comparison\n",
    "        fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
    "\n",
    "        models = list(self.evaluation_results.keys())\n",
    "\n",
    "        # Recall comparison\n",
    "        recalls = [self.evaluation_results[m]['recall_depressed'] for m in models]\n",
    "        axes[0, 0].barh(models, recalls, color='skyblue')\n",
    "        axes[0, 0].set_xlabel('Recall (Depressed)')\n",
    "        axes[0, 0].set_title('Recall: How many depressed cases caught?')\n",
    "        axes[0, 0].axvline(x=0.7, color='red', linestyle='--', label='Target 70%')\n",
    "        axes[0, 0].legend()\n",
    "\n",
    "        # Precision comparison\n",
    "        precisions = [self.evaluation_results[m]['precision_depressed'] for m in models]\n",
    "        axes[0, 1].barh(models, precisions, color='lightcoral')\n",
    "        axes[0, 1].set_xlabel('Precision (Depressed)')\n",
    "        axes[0, 1].set_title('Precision: How accurate are positive predictions?')\n",
    "\n",
    "        # MCC comparison\n",
    "        mccs = [self.evaluation_results[m]['mcc'] for m in models]\n",
    "        axes[1, 0].barh(models, mccs, color='lightgreen')\n",
    "        axes[1, 0].set_xlabel('Matthews Correlation Coefficient')\n",
    "        axes[1, 0].set_title('MCC: Overall quality metric')\n",
    "\n",
    "        # F1 comparison\n",
    "        f1s = [self.evaluation_results[m]['f1_score'] for m in models]\n",
    "        axes[1, 1].barh(models, f1s, color='plum')\n",
    "        axes[1, 1].set_xlabel('F1 Score')\n",
    "        axes[1, 1].set_title('F1: Balance of Precision and Recall')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(f'{self.output_dir}/visualizations/medical_metrics.png', dpi=150)\n",
    "        plt.close()\n",
    "        print(\"Saved medical metrics comparison\")\n",
    "\n",
    "    def save_best_model(self):\n",
    "        \"\"\"Save best model and generate detailed report\"\"\"\n",
    "        if self.best_model:\n",
    "            # Save model\n",
    "            path = f'{self.output_dir}/models/best_model.pkl'\n",
    "            joblib.dump(self.best_model, path)\n",
    "            print(f\"Saved best model to {path}\")\n",
    "\n",
    "            # Generate detailed report with UTF-8 encoding\n",
    "            report_path = f'{self.output_dir}/reports/best_model_details.txt'\n",
    "            with open(report_path, 'w', encoding='utf-8') as f:\n",
    "                f.write(\"=\"*70 + \"\\n\")\n",
    "                f.write(\"BEST MODEL EVALUATION REPORT\\n\")\n",
    "                f.write(\"=\"*70 + \"\\n\\n\")\n",
    "                f.write(f\"Generated: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\\n\\n\")\n",
    "\n",
    "                f.write(f\"Best Model: {self.best_model_name}\\n\")\n",
    "                f.write(f\"Parameters: {self.best_params[self.best_model_name]}\\n\\n\")\n",
    "\n",
    "                res = self.evaluation_results[self.best_model_name]\n",
    "\n",
    "                f.write(\"PERFORMANCE METRICS:\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                f.write(f\"Accuracy: {res['test_accuracy']:.4f}\\n\")\n",
    "                f.write(f\"F1 Score: {res['f1_score']:.4f}\\n\")\n",
    "                f.write(f\"Matthews Correlation Coefficient: {res['mcc']:.4f}\\n\")\n",
    "                f.write(f\"ROC AUC: {res['roc_auc']:.4f}\\n\\n\")\n",
    "\n",
    "                f.write(\"MEDICAL METRICS (Critical for Healthcare):\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                f.write(f\"Recall (Depressed): {res['recall_depressed']:.4f}\\n\")\n",
    "                f.write(f\"  >> Catches {res['recall_depressed']*100:.1f}% of depressed students\\n\")\n",
    "                f.write(f\"Precision (Depressed): {res['precision_depressed']:.4f}\\n\")\n",
    "                f.write(f\"  >> {res['precision_depressed']*100:.1f}% of positive predictions are correct\\n\")\n",
    "                f.write(f\"Specificity: {res['specificity']:.4f}\\n\")\n",
    "                f.write(f\"  >> Correctly identifies {res['specificity']*100:.1f}% of healthy students\\n\\n\")\n",
    "\n",
    "                f.write(\"CONFUSION MATRIX:\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                f.write(f\"True Negatives (Correctly identified healthy): {res['tn']}\\n\")\n",
    "                f.write(f\"False Positives (Healthy flagged as depressed): {res['fp']}\\n\")\n",
    "                f.write(f\"False Negatives (Depressed missed): {res['fn']} [WARNING]\\n\")\n",
    "                f.write(f\"True Positives (Correctly identified depressed): {res['tp']}\\n\\n\")\n",
    "\n",
    "                f.write(\"CLINICAL INTERPRETATION:\\n\")\n",
    "                f.write(\"-\" * 50 + \"\\n\")\n",
    "                if res['recall_depressed'] < 0.7:\n",
    "                    f.write(\"WARNING: Low recall for depressed class.\\n\")\n",
    "                    f.write(\"Model may miss many students needing help.\\n\")\n",
    "                    f.write(\"Recommendations:\\n\")\n",
    "                    f.write(\"  1. Collect more data, especially depressed cases\\n\")\n",
    "                    f.write(\"  2. Adjust decision threshold to favor recall\\n\")\n",
    "                    f.write(\"  3. Use ensemble methods or cost-sensitive learning\\n\")\n",
    "                else:\n",
    "                    f.write(\"GOOD: Acceptable recall - model catches most depressed cases.\\n\")\n",
    "\n",
    "                if res['precision_depressed'] < 0.5:\n",
    "                    f.write(\"\\nWARNING: Low precision - many false alarms.\\n\")\n",
    "                    f.write(\"Consider threshold adjustment or additional features.\\n\")\n",
    "\n",
    "                f.write(f\"\\nOverfitting Check: {res['overfit_gap']:.4f}\\n\")\n",
    "                if res['overfit_gap'] > 0.1:\n",
    "                    f.write(\"WARNING: Model may be overfitting (gap > 10%).\\n\")\n",
    "                else:\n",
    "                    f.write(\"GOOD: Good generalization.\\n\")\n",
    "\n",
    "            print(f\"Saved detailed report to {report_path}\")\n",
    "\n",
    "    def run_complete_evaluation(self, n_iter=15):\n",
    "        \"\"\"Run the complete evaluation pipeline\"\"\"\n",
    "        print(\"\\n\" + \"=\"*70)\n",
    "        print(\"MEDICAL DEPRESSION MODEL EVALUATION PIPELINE\")\n",
    "        print(\"=\"*70)\n",
    "\n",
    "        if not self.load_data():\n",
    "            return False\n",
    "\n",
    "        if not self.hyperparameter_tuning(n_iter=n_iter):\n",
    "            return False\n",
    "\n",
    "        self.evaluate_models()\n",
    "\n",
    "        if self.compare_models():\n",
    "            self.plot_visualizations()\n",
    "            self.save_best_model()\n",
    "            print(\"\\n\" + \"=\"*70)\n",
    "            print(\"PIPELINE COMPLETED SUCCESSFULLY!\")\n",
    "            print(\"=\"*70)\n",
    "            print(f\"\\nResults saved to: {self.output_dir}/\")\n",
    "            print(f\"   - Model comparison: model_comparison.csv\")\n",
    "            print(f\"   - Visualizations: visualizations/\")\n",
    "            print(f\"   - Best model: models/best_model.pkl\")\n",
    "            print(f\"   - Detailed report: reports/best_model_details.txt\")\n",
    "            return True\n",
    "\n",
    "        return False\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Ensure this matches your file path exactly\n",
    "    DATA_FILE = \"data/processed/preprocessed_data.csv\"\n",
    "\n",
    "    evaluator = ModelEvaluator(DATA_FILE)\n",
    "    # n_iter=15 is a good balance between speed and thorough search\n",
    "    evaluator.run_complete_evaluation(n_iter=15)"
   ],
   "id": "1108207a589bc763",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "MEDICAL DEPRESSION MODEL EVALUATION PIPELINE\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "LOADING DATA\n",
      "======================================================================\n",
      "Data loaded: (27901, 63)\n",
      "Checking target labels...\n",
      "  Raw unique values: [np.int64(0), np.int64(1)]\n",
      "  Final Distribution:\n",
      "Depression\n",
      "1    16336\n",
      "0    11565\n",
      "Name: count, dtype: int64\n",
      "Train shape: (22320, 62)\n",
      "Test shape: (5581, 62)\n",
      "Train class distribution: {1: 13068, 0: 9252}\n",
      "Test class distribution: {1: 3268, 0: 2313}\n",
      "\n",
      "======================================================================\n",
      "HYPERPARAMETER TUNING\n",
      "======================================================================\n",
      "\n",
      "Training Logistic Regression...\n",
      "  Success! Best F1 Score: 0.8715\n",
      "  Best Parameters: {'solver': 'liblinear', 'class_weight': None, 'C': 0.1}\n",
      "\n",
      "Training Random Forest...\n",
      "  Success! Best F1 Score: 0.8681\n",
      "  Best Parameters: {'n_estimators': 200, 'min_samples_leaf': 4, 'max_depth': 20, 'class_weight': None}\n",
      "\n",
      "Training Gradient Boosting...\n",
      "  Success! Best F1 Score: 0.8707\n",
      "  Best Parameters: {'n_estimators': 100, 'max_depth': 3, 'learning_rate': 0.2}\n",
      "\n",
      "Training K-Nearest Neighbors...\n",
      "  Success! Best F1 Score: 0.8516\n",
      "  Best Parameters: {'weights': 'distance', 'n_neighbors': 9}\n",
      "\n",
      "Training Decision Tree...\n",
      "  Success! Best F1 Score: 0.8484\n",
      "  Best Parameters: {'min_samples_leaf': 2, 'max_depth': 5, 'class_weight': None}\n",
      "\n",
      "Training Naive Bayes...\n",
      "  Success! Best F1 Score: 0.0050\n",
      "  Best Parameters: {'var_smoothing': 1e-09}\n",
      "\n",
      "Successfully trained 6 models\n",
      "\n",
      "======================================================================\n",
      "EVALUATING MODELS (Medical Metrics)\n",
      "======================================================================\n",
      "\n",
      "Logistic Regression:\n",
      "  Accuracy: 0.846\n",
      "  F1 Score: 0.870\n",
      "  MCC: 0.680\n",
      "  Recall (Depressed): 0.882 [CRITICAL]\n",
      "  Precision (Depressed): 0.858\n",
      "  Specificity: 0.793\n",
      "\n",
      "Random Forest:\n",
      "  Accuracy: 0.838\n",
      "  F1 Score: 0.865\n",
      "  MCC: 0.664\n",
      "  Recall (Depressed): 0.885 [CRITICAL]\n",
      "  Precision (Depressed): 0.845\n",
      "  Specificity: 0.771\n",
      "\n",
      "Gradient Boosting:\n",
      "  Accuracy: 0.843\n",
      "  F1 Score: 0.867\n",
      "  MCC: 0.674\n",
      "  Recall (Depressed): 0.880 [CRITICAL]\n",
      "  Precision (Depressed): 0.855\n",
      "  Specificity: 0.789\n",
      "\n",
      "K-Nearest Neighbors:\n",
      "  Accuracy: 0.824\n",
      "  F1 Score: 0.855\n",
      "  MCC: 0.635\n",
      "  Recall (Depressed): 0.883 [CRITICAL]\n",
      "  Precision (Depressed): 0.828\n",
      "  Specificity: 0.741\n",
      "\n",
      "Decision Tree:\n",
      "  Accuracy: 0.813\n",
      "  F1 Score: 0.840\n",
      "  MCC: 0.614\n",
      "  Recall (Depressed): 0.841 [CRITICAL]\n",
      "  Precision (Depressed): 0.840\n",
      "  Specificity: 0.773\n",
      "\n",
      "Naive Bayes:\n",
      "  Accuracy: 0.415\n",
      "  F1 Score: 0.004\n",
      "  MCC: 0.007\n",
      "  Recall (Depressed): 0.002 [CRITICAL]\n",
      "  Precision (Depressed): 0.667\n",
      "  Specificity: 0.999\n",
      "\n",
      "======================================================================\n",
      "FINAL RESULTS - MODEL COMPARISON\n",
      "======================================================================\n",
      "\n",
      "COMPLETE MODEL COMPARISON:\n",
      "              Model  Accuracy       F1      MCC  Recall_Depressed  Precision_Depressed  Specificity  ROC_AUC  Overfit_Gap\n",
      "Logistic Regression  0.845547 0.869985 0.680291          0.882497             0.857823     0.793342 0.918514     0.002436\n",
      "  Gradient Boosting  0.842501 0.867441 0.673962          0.880049             0.855189     0.789451 0.919008     0.014578\n",
      "      Random Forest  0.837843 0.864744 0.663708          0.885251             0.845165     0.770860 0.915808     0.044953\n",
      "K-Nearest Neighbors  0.824225 0.854774 0.634903          0.883415             0.827932     0.740597 0.892814     0.175775\n",
      "      Decision Tree  0.812937 0.840367 0.614492          0.840881             0.839853     0.773454 0.886371     0.015020\n",
      "        Naive Bayes  0.414979 0.003662 0.006617          0.001836             0.666667     0.998703 0.847975     0.000702\n",
      "\n",
      "WINNER: Logistic Regression\n",
      "   F1 Score: 0.870\n",
      "   Recall (Depressed): 0.882\n",
      "   MCC: 0.680\n",
      "\n",
      "Good recall (88.2%) - Model catches most depressed cases\n",
      "Saved confusion matrices\n",
      "Saved ROC curves\n",
      "Saved medical metrics comparison\n",
      "Saved best model to evaluation_results/models/best_model.pkl\n",
      "Saved detailed report to evaluation_results/reports/best_model_details.txt\n",
      "\n",
      "======================================================================\n",
      "PIPELINE COMPLETED SUCCESSFULLY!\n",
      "======================================================================\n",
      "\n",
      "Results saved to: evaluation_results/\n",
      "   - Model comparison: model_comparison.csv\n",
      "   - Visualizations: visualizations/\n",
      "   - Best model: models/best_model.pkl\n",
      "   - Detailed report: reports/best_model_details.txt\n"
     ]
    }
   ],
   "execution_count": 2
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
